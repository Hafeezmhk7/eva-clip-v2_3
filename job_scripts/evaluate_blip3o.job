#!/bin/bash
#SBATCH --job-name=blip3o_coco_eval_precomputed
#SBATCH --partition=gpu_h100
#SBATCH --nodes=1
#SBATCH --gpus=1
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=8
#SBATCH --time=2:00:00
#SBATCH --mem=32G
#SBATCH --output=./slurm_out/blip3o_coco_eval_precomputed_%j.out
#SBATCH --error=./slurm_out/blip3o_coco_eval_precomputed_%j.err

# =============================================================================
# BLIP3-o MS-COCO Evaluation with Pre-computed Embeddings
# Memory-efficient evaluation using pre-extracted CLIP and EVA embeddings
# =============================================================================

echo "üî¨ BLIP3-o MS-COCO Evaluation with Pre-computed Embeddings"
echo "=================================================================="
echo "Job ID: ${SLURM_JOB_ID}"
echo "Node: $(hostname)"
echo "Time: $(date)"
echo "GPU: $(nvidia-smi --query-gpu=name --format=csv,noheader,nounits | head -1)"
echo "=================================================================="

cd $SLURM_SUBMIT_DIR

# Setup environment
module purge
module load 2024
module load Miniconda3/24.7.1-0
module load CUDA/12.6.0
source activate eva_clip_env

# Configuration - can be overridden by command line arguments
MODEL_PATH="/home/azadaianchuk1/eva-clip-v6_2/checkpoints/blip3o_clean_20250803_015324"
COCO_EMBEDDINGS_FILE=""
OUTPUT_DIR="./coco_eval_precomputed_$(date +%Y%m%d_%H%M%S)"

# Training embeddings directory (for normalization)
TRAINING_EMBEDDINGS_DIR="/scratch-local/azadaianchuk1.13526739/blip3o_job_13526739/working/coco_embeddings/"

# Evaluation parameters
MAX_SAMPLES=1000  # Set to None or remove for all samples
BATCH_SIZE=8
NUM_INFERENCE_STEPS=50
USE_HEUN=true

# Parse command line arguments
if [ -n "$1" ]; then
    MODEL_PATH="$1"
    echo "üìÇ Using provided model path: $MODEL_PATH"
else
    echo "üìÇ Using default model path: $MODEL_PATH"
    # Auto-detect most recent model if default doesn't exist
    if [ ! -d "$MODEL_PATH" ]; then
        echo "üîç Auto-detecting most recent model..."
        MODEL_PATH=$(find ./checkpoints -name "*blip3o*" -type d | sort | tail -1)
        if [ -z "$MODEL_PATH" ]; then
            echo "‚ùå No model found! Please provide model path as first argument."
            echo "Usage: sbatch job_scripts/evaluate_blip3o_precomputed.job <model_path> <embeddings_file>"
            exit 1
        fi
        echo "üìÇ Found model: $MODEL_PATH"
    fi
fi

if [ -n "$2" ]; then
    COCO_EMBEDDINGS_FILE="$2"
    echo "üìÇ Using provided embeddings file: $COCO_EMBEDDINGS_FILE"
else
    echo "üîç Auto-detecting COCO embeddings file..."
    
    # Look for consolidated embeddings file in various locations
    SEARCH_PATHS=(
        "./coco_embeddings/coco_embeddings_consolidated.pkl"
        "./coco_embeddings_consolidated.pkl"
        "*/coco_embeddings_consolidated.pkl"
    )
    
    # Try temp manager working directory if available
    if [ -n "$BLIP3O_WORKSPACE" ]; then
        SEARCH_PATHS+=("$BLIP3O_WORKSPACE/*/coco_embeddings_consolidated.pkl")
    fi
    
    for path in "${SEARCH_PATHS[@]}"; do
        if [ -f "$path" ]; then
            COCO_EMBEDDINGS_FILE="$path"
            echo "‚úÖ Found embeddings file: $COCO_EMBEDDINGS_FILE"
            break
        fi
    done
    
    # If not found, look for any embeddings file
    if [ -z "$COCO_EMBEDDINGS_FILE" ]; then
        COCO_EMBEDDINGS_FILE=$(find . -name "coco_embeddings_consolidated.pkl" 2>/dev/null | head -1)
        if [ -n "$COCO_EMBEDDINGS_FILE" ]; then
            echo "‚úÖ Found embeddings file: $COCO_EMBEDDINGS_FILE"
        fi
    fi
    
    if [ -z "$COCO_EMBEDDINGS_FILE" ]; then
        echo "‚ùå No COCO embeddings file found!"
        echo ""
        echo "üí° Please extract COCO embeddings first:"
        echo "   sbatch job_scripts/extract_coco_embeddings.job"
        echo ""
        echo "Or provide embeddings file as second argument:"
        echo "   sbatch job_scripts/evaluate_blip3o_precomputed.job <model_path> <embeddings_file>"
        exit 1
    fi
fi

# Training embeddings dir can be provided as third argument
if [ -n "$3" ]; then
    TRAINING_EMBEDDINGS_DIR="$3"
    echo "üìÇ Using provided training embeddings dir: $TRAINING_EMBEDDINGS_DIR"
fi

# Max samples can be provided as fourth argument
if [ -n "$4" ]; then
    MAX_SAMPLES="$4"
    echo "üìä Using provided max samples: $MAX_SAMPLES"
fi

# Create output directory
mkdir -p "${OUTPUT_DIR}"
mkdir -p ./slurm_out

echo ""
echo "‚öôÔ∏è Memory-Efficient MS-COCO Evaluation Configuration:"
echo "=========================================="
echo "Model: $MODEL_PATH"
echo "COCO Embeddings: $COCO_EMBEDDINGS_FILE"
echo "Training Embeddings: $TRAINING_EMBEDDINGS_DIR"
echo "Output: $OUTPUT_DIR"
echo "Max Samples: ${MAX_SAMPLES:-All}"
echo "Batch Size: $BATCH_SIZE"
echo "Inference Steps: $NUM_INFERENCE_STEPS"
echo "Use Heun Solver: $USE_HEUN"
echo ""
echo "üöÄ Memory-Efficient Strategy:"
echo "   ‚Ä¢ Pre-computed embeddings avoid real-time extraction"
echo "   ‚Ä¢ No CLIP/EVA model loading during evaluation"
echo "   ‚Ä¢ Lower GPU memory usage"
echo "   ‚Ä¢ Faster evaluation"
echo ""

# Verify model exists
if [ ! -d "$MODEL_PATH" ]; then
    echo "‚ùå Model directory not found: $MODEL_PATH"
    echo "Available models:"
    ls -la ./checkpoints/ 2>/dev/null || echo "No checkpoints found"
    exit 1
fi

# Check for required model files
MODEL_CONFIG="$MODEL_PATH/config.json"
MODEL_WEIGHTS_SAFETENSORS="$MODEL_PATH/model.safetensors"
MODEL_WEIGHTS_BIN="$MODEL_PATH/pytorch_model.bin"
CHECKPOINT_FILES="$MODEL_PATH/checkpoint_step_*.pt"

# Check what format the model is in
if [ -f "$MODEL_CONFIG" ] && ([ -f "$MODEL_WEIGHTS_SAFETENSORS" ] || [ -f "$MODEL_WEIGHTS_BIN" ]); then
    echo "‚úÖ HuggingFace format model detected"
elif ls $CHECKPOINT_FILES 1> /dev/null 2>&1; then
    echo "‚úÖ Checkpoint format model detected"
    LATEST_CHECKPOINT=$(ls -t $CHECKPOINT_FILES | head -1)
    echo "   Latest checkpoint: $LATEST_CHECKPOINT"
else
    echo "‚ùå No valid model files found in: $MODEL_PATH"
    echo "Expected:"
    echo "  - HuggingFace format: config.json + (model.safetensors OR pytorch_model.bin)"
    echo "  - Checkpoint format: checkpoint_step_*.pt"
    echo "Model directory contents:"
    ls -la "$MODEL_PATH"
    exit 1
fi

echo "‚úÖ Model verified: $MODEL_PATH"

# Verify COCO embeddings file
if [ ! -f "$COCO_EMBEDDINGS_FILE" ]; then
    echo "‚ùå COCO embeddings file not found: $COCO_EMBEDDINGS_FILE"
    echo ""
    echo "üí° To extract COCO embeddings:"
    echo "   sbatch job_scripts/extract_coco_embeddings.job"
    exit 1
fi

# Check embeddings file size and details
EMBEDDINGS_SIZE=$(stat --format="%s" "$COCO_EMBEDDINGS_FILE" 2>/dev/null || echo "0")
EMBEDDINGS_SIZE_GB=$(echo "scale=2; $EMBEDDINGS_SIZE / 1024 / 1024 / 1024" | bc -l 2>/dev/null || echo "?.??")

echo "‚úÖ COCO embeddings file verified:"
echo "   File: $COCO_EMBEDDINGS_FILE"
echo "   Size: ${EMBEDDINGS_SIZE_GB} GB"

# Try to get embeddings info if manifest exists
MANIFEST_FILE="${COCO_EMBEDDINGS_FILE%.*}_manifest.json"
if [ -f "$MANIFEST_FILE" ]; then
    echo "‚úÖ Manifest file found: $MANIFEST_FILE"
    
    # Extract info from manifest
    python -c "
import json
import sys
try:
    with open('$MANIFEST_FILE', 'r') as f:
        manifest = json.load(f)
    
    print(f'üìä Embeddings Details:')
    print(f'   Total samples: {manifest.get(\"total_samples\", \"unknown\"):,}')
    print(f'   CLIP shape: {manifest.get(\"clip_shape\", \"unknown\")}')
    print(f'   EVA shape: {manifest.get(\"eva_shape\", \"unknown\")}')
    print(f'   Include CLS: {manifest.get(\"include_cls\", \"unknown\")}')
    print(f'   Tokens per sample: {manifest.get(\"tokens\", \"unknown\")}')
    print(f'   Created: {manifest.get(\"created\", \"unknown\")}')
    
except Exception as e:
    print(f'Could not parse manifest: {e}')
    sys.exit(1)
"
else
    echo "‚ö†Ô∏è No manifest file found - will inspect embeddings during evaluation"
fi

# Check training embeddings directory
echo ""
echo "üîç Verifying training embeddings directory for normalization..."
if [ ! -d "$TRAINING_EMBEDDINGS_DIR" ]; then
    echo "‚ùå Training embeddings directory not found: $TRAINING_EMBEDDINGS_DIR"
    echo ""
    echo "‚ö†Ô∏è IMPORTANT: Training embeddings are needed for accurate normalization!"
    echo ""
    echo "Expected structure:"
    echo "  $TRAINING_EMBEDDINGS_DIR/"
    echo "  ‚îú‚îÄ‚îÄ embeddings_shard_00000_patch_only.pkl"
    echo "  ‚îú‚îÄ‚îÄ embeddings_shard_00001_patch_only.pkl"
    echo "  ‚îî‚îÄ‚îÄ ..."
    echo ""
    echo "Options:"
    echo "  1. Provide correct path as 3rd argument"
    echo "  2. Evaluation will use approximate fallback normalization"
    echo ""
    echo "‚ö†Ô∏è Proceeding with fallback - results may not be fully accurate"
    TRAINING_EMBEDDINGS_STATUS="‚ùå NOT FOUND (using fallback)"
else
    # Check for .pkl files
    PKL_COUNT=$(find "$TRAINING_EMBEDDINGS_DIR" -name "*.pkl" | wc -l)
    if [ $PKL_COUNT -eq 0 ]; then
        echo "‚ùå No .pkl files found in training embeddings directory"
        echo "Directory contents:"
        ls -la "$TRAINING_EMBEDDINGS_DIR"
        TRAINING_EMBEDDINGS_STATUS="‚ùå NO FILES (using fallback)"
    else
        echo "‚úÖ Training embeddings directory verified:"
        echo "   Directory: $TRAINING_EMBEDDINGS_DIR"
        echo "   Found: $PKL_COUNT .pkl files"
        echo "   üìä Will use training statistics for normalization"
        TRAINING_EMBEDDINGS_STATUS="‚úÖ AVAILABLE (exact statistics)"
    fi
fi

# Verify evaluation script
if [ ! -f "eval_blip3o_coco_updated.py" ]; then
    echo "‚ùå Evaluation script not found: eval_blip3o_coco_updated.py"
    echo "Please ensure the updated evaluation script is in the current directory"
    exit 1
fi

echo "‚úÖ Evaluation script found"

echo ""
echo "üöÄ Starting Memory-Efficient MS-COCO Evaluation..."
echo "========================================"
echo "üéØ Objective: Evaluate model using pre-computed embeddings"
echo "üíæ Strategy: Load embeddings from disk instead of real-time extraction"
echo "üî• Benefits:"
echo "   ‚Ä¢ No GPU memory for CLIP/EVA models"
echo "   ‚Ä¢ Faster evaluation (no re-extraction)"
echo "   ‚Ä¢ Consistent embeddings across evaluations"
echo "   ‚Ä¢ Higher batch sizes possible"
echo ""
echo "Embeddings: Pre-computed from MS-COCO val2017"
echo "Training Statistics: $TRAINING_EMBEDDINGS_STATUS"
echo "Expected Performance: ~0.912 (training reference)"
echo ""

# Build evaluation command
EVAL_CMD="python eval_blip3o_coco_updated.py \
    --model_path \"$MODEL_PATH\" \
    --coco_embeddings_file \"$COCO_EMBEDDINGS_FILE\" \
    --training_embeddings_dir \"$TRAINING_EMBEDDINGS_DIR\" \
    --output_dir \"$OUTPUT_DIR\" \
    --batch_size $BATCH_SIZE \
    --num_inference_steps $NUM_INFERENCE_STEPS \
    --save_results"

# Add optional parameters
if [ "$USE_HEUN" = true ]; then
    EVAL_CMD="$EVAL_CMD --use_heun"
fi

if [ -n "$MAX_SAMPLES" ] && [ "$MAX_SAMPLES" != "None" ]; then
    EVAL_CMD="$EVAL_CMD --max_samples $MAX_SAMPLES"
fi

echo "Executing: $EVAL_CMD"
echo ""

# Monitor initial GPU memory
echo "üìä Initial GPU Memory Status:"
nvidia-smi --query-gpu=name,memory.total,memory.used,memory.free --format=csv,noheader,nounits | \
    awk '{printf "GPU: %s | Total: %s MB | Used: %s MB | Free: %s MB\n", $1, $2, $3, $4}'

echo ""

# Run evaluation
eval $EVAL_CMD

EVAL_EXIT_CODE=$?

echo ""
echo "=================================================================="
echo "üìä Memory-Efficient MS-COCO Evaluation Results"
echo "=================================================================="

if [ $EVAL_EXIT_CODE -eq 0 ]; then
    echo "‚úÖ Memory-efficient MS-COCO evaluation completed successfully!"
    
    # Find and display results
    RESULTS_FILE="$OUTPUT_DIR/coco_evaluation_precomputed.json"
    
    if [ -f "$RESULTS_FILE" ]; then
        echo ""
        echo "üìã Results Summary (Memory-Efficient Evaluation):"
        echo "=============================================="
        
        # Extract key metrics using Python
        python -c "
import json
import sys
try:
    with open('$RESULTS_FILE', 'r') as f:
        metrics = json.load(f)
    
    # Main results
    current_sim = metrics.get('eval_clip_similarity', 0)
    training_ref = 0.912  # From training logs
    diff_pct = ((current_sim - training_ref) / training_ref) * 100 if training_ref > 0 else 0
    
    print(f'üéØ Current CLIP Similarity: {current_sim:.4f}')
    print(f'üìö Training Reference: {training_ref:.4f}')
    print(f'üìä Difference: {diff_pct:+.1f}%')
    
    # Status assessment
    if abs(diff_pct) < 5:
        print(f'‚úÖ STATUS: Very close to training performance!')
    elif abs(diff_pct) < 15:
        print(f'üìä STATUS: Reasonably close to training performance')
    else:
        print(f'‚ö†Ô∏è STATUS: Significant difference from training')
    
    print(f'')
    print(f'üíæ Memory Efficiency Status:')
    memory_efficient = metrics.get('memory_efficient', False)
    print(f'   Memory-Efficient Mode: {\"‚úÖ ENABLED\" if memory_efficient else \"‚ùå DISABLED\"}')
    
    if memory_efficient:
        print(f'   üìÇ Embeddings Source: Pre-computed file')
        print(f'   üöÄ Benefits: No real-time extraction, lower GPU usage')
    
    print(f'')
    print(f'üèÜ Quality Distribution:')
    print(f'   High Quality (>0.7): {metrics.get(\"eval_high_quality\", 0)*100:.1f}%')
    print(f'   Very High Quality (>0.8): {metrics.get(\"eval_very_high_quality\", 0)*100:.1f}%')
    print(f'   Excellent Quality (>0.9): {metrics.get(\"eval_excellent_quality\", 0)*100:.1f}%')
    
    print(f'')
    print(f'üìä Evaluation Details:')
    print(f'   Samples Evaluated: {metrics.get(\"eval_samples\", 0):,}')
    print(f'   Success Rate: {metrics.get(\"eval_success_rate\", 0)*100:.1f}%')
    print(f'   Evaluation Time: {metrics.get(\"evaluation_time_seconds\", 0):.1f} seconds')
    print(f'   Inference Steps: {metrics.get(\"inference_steps\", 0)}')
    print(f'   Solver: {\"Heun\" if metrics.get(\"use_heun_solver\", False) else \"Euler\"}')
    
    # Normalization status
    training_stats = metrics.get('training_statistics_applied', False)
    denorm_applied = metrics.get('denormalization_applied', False)
    print(f'')
    print(f'üîß Normalization Status:')
    print(f'   Training Statistics: {\"‚úÖ APPLIED\" if training_stats else \"‚ùå NOT APPLIED\"}')
    print(f'   Denormalization: {\"‚úÖ APPLIED\" if denorm_applied else \"‚ùå NOT APPLIED\"}')
    
    # Memory comparison
    print(f'')
    if memory_efficient:
        print(f'üéâ MEMORY EFFICIENCY SUCCESS!')
        print(f'   ‚úÖ No CLIP/EVA model loading during evaluation')
        print(f'   ‚úÖ Pre-computed embeddings used')
        print(f'   ‚úÖ Lower GPU memory requirements')
        print(f'   ‚úÖ Faster evaluation')
        
        if abs(diff_pct) < 10:
            print(f'   üéØ Performance maintained with memory efficiency!')
        
    # Performance assessment
    print(f'')
    if training_stats and denorm_applied:
        if abs(diff_pct) < 5:
            print(f'üéâ ASSESSMENT: EXCELLENT - Memory-efficient evaluation matches training!')
            print(f'   The model performs very similarly to training with much lower memory usage.')
        elif abs(diff_pct) < 15:
            print(f'‚úÖ ASSESSMENT: GOOD - Close performance with memory efficiency')
            print(f'   Small differences while achieving significant memory savings.')
        else:
            print(f'‚ö†Ô∏è ASSESSMENT: INVESTIGATION NEEDED - Check normalization')
            print(f'   Consider verifying training statistics integration.')
    else:
        print(f'‚ö†Ô∏è ASSESSMENT: Incomplete training integration')
        print(f'   Results may not be fully comparable to training performance.')
        
except Exception as e:
    print(f'Could not parse results: {e}')
    sys.exit(1)
"
        
        echo ""
        echo "üìÅ Results saved to: $OUTPUT_DIR"
        echo "üìÑ Full results: $RESULTS_FILE"
        
        # Check for plots
        PLOTS_FILE="$OUTPUT_DIR/coco_evaluation_precomputed.png"
        if [ -f "$PLOTS_FILE" ]; then
            echo "üìä Visualizations: $PLOTS_FILE"
        fi
    else
        echo "‚ö†Ô∏è No results file found"
        echo "Checking output directory contents:"
        ls -la "$OUTPUT_DIR" 2>/dev/null || echo "Output directory not found"
    fi
    
    echo ""
    echo "üéØ Memory-Efficient Evaluation Benefits:"
    echo "======================================"
    echo "‚úÖ Avoided loading CLIP and EVA models during evaluation"
    echo "‚úÖ Used pre-computed embeddings from disk"
    echo "‚úÖ Lower GPU memory requirements"
    echo "‚úÖ Faster evaluation (no re-extraction)"
    echo "‚úÖ Consistent embeddings across multiple evaluations"
    echo "‚úÖ Enables larger batch sizes"
    
    echo ""
    echo "üí° Next Steps:"
    if [ -f "$RESULTS_FILE" ]; then
        python -c "
import json
try:
    with open('$RESULTS_FILE', 'r') as f:
        metrics = json.load(f)
    similarity = metrics.get('eval_clip_similarity', 0)
    
    if similarity > 0.8:
        print('‚Ä¢ EXCELLENT performance! Consider:')
        print('  - Running full evaluation (remove --max_samples)')
        print('  - Testing on additional datasets')
        print('  - Deploying the model')
    elif similarity > 0.6:
        print('‚Ä¢ GOOD performance! To improve:')
        print('  - Train with more data or longer')
        print('  - Fine-tune hyperparameters')
        print('  - Check model capacity')
    elif similarity > 0.4:
        print('‚Ä¢ FAIR performance! Consider:')
        print('  - Reviewing training setup')
        print('  - Checking normalization')
        print('  - Increasing model size')
    else:
        print('‚Ä¢ Performance needs investigation:')
        print('  - Verify model loading')
        print('  - Check normalization statistics')
        print('  - Review training logs')
except:
    print('‚Ä¢ Review detailed results in output directory')
"
    fi
    
    echo ""
    echo "‚úÖ SUCCESS: Memory-efficient MS-COCO evaluation completed!"
    
else
    echo "‚ùå FAILED: Evaluation exit code $EVAL_EXIT_CODE"
    echo ""
    echo "üí° Troubleshooting (Memory-Efficient Mode):"
    echo "========================================"
    echo "‚Ä¢ Check log files in ./slurm_out/ for detailed error messages"
    echo "‚Ä¢ Verify pre-computed embeddings file:"
    echo "  - File exists: $COCO_EMBEDDINGS_FILE"
    echo "  - File is not corrupted"
    echo "  - Embeddings format is correct"
    echo "‚Ä¢ Verify model path and checkpoint format"
    echo "‚Ä¢ Check training embeddings directory for normalization"
    echo "‚Ä¢ Monitor GPU memory - even with pre-computed embeddings, inference needs memory"
    echo ""
    echo "üîß Quick fixes:"
    echo "‚Ä¢ Reduce batch size: --batch_size 2"
    echo "‚Ä¢ Reduce samples: --max_samples 100"
    echo "‚Ä¢ Check embeddings file:"
    echo "  python -c \"import pickle; print(pickle.load(open('$COCO_EMBEDDINGS_FILE', 'rb')).keys())\""
    echo ""
    echo "üî• Memory-Efficient Specific Issues:"
    echo "‚Ä¢ If embeddings file is corrupted, re-extract:"
    echo "  sbatch job_scripts/extract_coco_embeddings.job"
    echo "‚Ä¢ If model loading fails, check checkpoint format"
    echo "‚Ä¢ If normalization fails, verify training embeddings directory"
fi

# Final GPU memory status
echo ""
echo "üìä Final GPU Memory Status:"
nvidia-smi --query-gpu=name,memory.total,memory.used,memory.free --format=csv,noheader,nounits | \
    awk '{printf "GPU: %s | Total: %s MB | Used: %s MB | Free: %s MB\n", $1, $2, $3, $4}'

echo ""
echo "üèÅ Job completed at $(date)"
echo "=================================================================="

# Show memory usage comparison
echo ""
echo "üìö MEMORY-EFFICIENT EVALUATION SUMMARY:"
echo "This evaluation uses pre-computed CLIP and EVA embeddings to avoid GPU memory issues."
echo "Benefits:"
echo "  ‚Ä¢ No real-time CLIP/EVA model loading (saves ~10-15GB GPU memory)"
echo "  ‚Ä¢ Faster evaluation (no re-extraction of embeddings)"
echo "  ‚Ä¢ Consistent embeddings across multiple evaluations"
echo "  ‚Ä¢ Enables larger batch sizes and more samples"
echo "  ‚Ä¢ Same evaluation quality as real-time extraction"
echo ""
echo "Memory savings: Estimated 10-15GB GPU memory compared to real-time extraction"
echo "=================================================================="

exit $EVAL_EXIT_CODE