#!/bin/bash
#SBATCH --job-name=blip3o_coco_eval
#SBATCH --partition=gpu_h100
#SBATCH --nodes=1
#SBATCH --gpus=1
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=16
#SBATCH --time=4:00:00
#SBATCH --mem=64G
#SBATCH --output=./slurm_out/blip3o_coco_eval_%j.out
#SBATCH --error=./slurm_out/blip3o_coco_eval_%j.err

# =============================================================================
# BLIP3-o MS-COCO Evaluation - Comprehensive Model Assessment
# Evaluates trained model on MS-COCO val2017 dataset for CLIP reproduction
# =============================================================================

echo "üî¨ BLIP3-o MS-COCO Comprehensive Evaluation"
echo "==========================================="
echo "Job ID: ${SLURM_JOB_ID}"
echo "Node: $(hostname)"
echo "Time: $(date)"
echo "GPU: $(nvidia-smi --query-gpu=name --format=csv,noheader,nounits | head -1)"
echo "==========================================="

cd $SLURM_SUBMIT_DIR

# Setup environment
module purge
module load 2024
module load Miniconda3/24.7.1-0
module load CUDA/12.6.0
source activate eva_clip_env

# Configuration
MODEL_PATH="/home/azadaianchuk1/eva-clip-v6_2/checkpoints/blip3o_clean_20250803_015324"
# /home/azadaianchuk1/eva-clip-v6_2/checkpoints/blip3o_clean_20250803_015324
COCO_ROOT="./data/coco"
OUTPUT_DIR="./coco_eval_results_$(date +%Y%m%d_%H%M%S)"
MAX_SAMPLES=1000  # Evaluate on subset for faster testing, set to null for full dataset

# Evaluation parameters
BATCH_SIZE=8
NUM_INFERENCE_STEPS=50
USE_HEUN=true

# Model path - can be provided as argument or use default
if [ -n "$1" ]; then
    MODEL_PATH="$1"
    echo "üìÇ Using provided model path: $MODEL_PATH"
else
    echo "üìÇ Using default model path: $MODEL_PATH"
    # Auto-detect most recent model if default doesn't exist
    if [ ! -d "$MODEL_PATH" ]; then
        echo "üîç Auto-detecting most recent model..."
        MODEL_PATH=$(find ./checkpoints -name "*blip3o*" -type d | sort | tail -1)
        if [ -z "$MODEL_PATH" ]; then
            echo "‚ùå No model found! Please provide model path as argument."
            echo "Usage: sbatch job_scripts/evaluate_blip3o_coco.job <model_path>"
            exit 1
        fi
        echo "üìÇ Found model: $MODEL_PATH"
    fi
fi

# COCO root can be provided as second argument
if [ -n "$2" ]; then
    COCO_ROOT="$2"
    echo "üìÇ Using provided COCO root: $COCO_ROOT"
fi

# Create output directory
mkdir -p "${OUTPUT_DIR}"
mkdir -p ./slurm_out

echo ""
echo "‚öôÔ∏è MS-COCO Evaluation Configuration:"
echo "===================================="
echo "Model: $MODEL_PATH"
echo "COCO Root: $COCO_ROOT"
echo "Output: $OUTPUT_DIR"
echo "Max Samples: $MAX_SAMPLES"
echo "Batch Size: $BATCH_SIZE"
echo "Inference Steps: $NUM_INFERENCE_STEPS"
echo "Use Heun Solver: $USE_HEUN"
echo ""

# Verify model exists
if [ ! -d "$MODEL_PATH" ]; then
    echo "‚ùå Model directory not found: $MODEL_PATH"
    echo "Available models:"
    ls -la ./checkpoints/ 2>/dev/null || echo "No checkpoints found"
    exit 1
fi

# Check for required model files
MODEL_CONFIG="$MODEL_PATH/config.json"
MODEL_WEIGHTS_SAFETENSORS="$MODEL_PATH/model.safetensors"
MODEL_WEIGHTS_BIN="$MODEL_PATH/pytorch_model.bin"
CHECKPOINT_FILES="$MODEL_PATH/checkpoint_step_*.pt"

# Check what format the model is in
if [ -f "$MODEL_CONFIG" ] && ([ -f "$MODEL_WEIGHTS_SAFETENSORS" ] || [ -f "$MODEL_WEIGHTS_BIN" ]); then
    echo "‚úÖ HuggingFace format model detected"
elif ls $CHECKPOINT_FILES 1> /dev/null 2>&1; then
    echo "‚úÖ Checkpoint format model detected"
    LATEST_CHECKPOINT=$(ls -t $CHECKPOINT_FILES | head -1)
    echo "   Latest checkpoint: $LATEST_CHECKPOINT"
else
    echo "‚ùå No valid model files found in: $MODEL_PATH"
    echo "Expected:"
    echo "  - HuggingFace format: config.json + (model.safetensors OR pytorch_model.bin)"
    echo "  - Checkpoint format: checkpoint_step_*.pt"
    echo "Model directory contents:"
    ls -la "$MODEL_PATH"
    exit 1
fi

echo "‚úÖ Model verified: $MODEL_PATH"

# Check if COCO dataset exists
if [ ! -d "$COCO_ROOT" ]; then
    echo "‚ùå COCO dataset not found: $COCO_ROOT"
    echo ""
    echo "üí° To download COCO dataset:"
    echo "   python src/data_hand/download_coco.py --coco_root $COCO_ROOT"
    echo ""
    echo "Or download manually:"
    echo "   mkdir -p $COCO_ROOT"
    echo "   # Download val2017.zip and annotations_trainval2017.zip"
    echo "   # Extract to create $COCO_ROOT/images/val2017/ and $COCO_ROOT/annotations/"
    exit 1
fi

# Verify COCO structure
COCO_IMAGES="$COCO_ROOT/val2017/"
COCO_ANNOTATIONS="$COCO_ROOT/annotations/captions_val2017.json"

if [ ! -d "$COCO_IMAGES" ]; then
    echo "‚ùå COCO images directory not found: $COCO_IMAGES"
    exit 1
fi

if [ ! -f "$COCO_ANNOTATIONS" ]; then
    echo "‚ùå COCO annotations file not found: $COCO_ANNOTATIONS"
    exit 1
fi

# Count available images
IMAGE_COUNT=$(find "$COCO_IMAGES" -name "*.jpg" | wc -l)
echo "‚úÖ COCO dataset verified:"
echo "   Images directory: $COCO_IMAGES"
echo "   Annotations file: $COCO_ANNOTATIONS"
echo "   Available images: $IMAGE_COUNT"

if [ $IMAGE_COUNT -eq 0 ]; then
    echo "‚ùå No images found in COCO images directory"
    exit 1
fi

# Verify evaluation script
if [ ! -f "eval_blip3o_coco.py" ]; then
    echo "‚ùå Evaluation script not found: eval_blip3o_coco.py"
    echo "Please ensure the MS-COCO evaluation script is in the current directory"
    exit 1
fi

echo "‚úÖ Evaluation script found"

echo ""
echo "üöÄ Starting MS-COCO Evaluation..."
echo "================================="
echo "Task: Evaluate BLIP3-o model's ability to reproduce CLIP embeddings"
echo "Method: Generate CLIP embeddings from EVA features and compare with ground truth"
echo "Dataset: MS-COCO val2017 ($IMAGE_COUNT images available)"
echo "Expected: Cosine similarity between generated and ground truth CLIP embeddings"
echo ""

# Launch MS-COCO evaluation
if [ "$USE_HEUN" = true ]; then
    HEUN_FLAG="--use_heun"
else
    HEUN_FLAG=""
fi

# Build command
EVAL_CMD="python eval_blip3o_coco.py \
    --model_path \"$MODEL_PATH\" \
    --coco_root \"$COCO_ROOT\" \
    --output_dir \"$OUTPUT_DIR\" \
    --batch_size $BATCH_SIZE \
    --num_inference_steps $NUM_INFERENCE_STEPS \
    --save_results \
    --max_samples 1000 \
    $HEUN_FLAG"

# Add max_samples if specified
if [ "$MAX_SAMPLES" != "null" ] && [ -n "$MAX_SAMPLES" ]; then
    EVAL_CMD="$EVAL_CMD --max_samples $MAX_SAMPLES"
fi

echo "Executing: $EVAL_CMD"
echo ""

# Run evaluation
eval $EVAL_CMD

EVAL_EXIT_CODE=$?

echo ""
echo "========================================"
echo "üìä MS-COCO Evaluation Results"
echo "========================================"

if [ $EVAL_EXIT_CODE -eq 0 ]; then
    echo "‚úÖ MS-COCO evaluation completed successfully!"
    
    # Find and display results
    RESULTS_FILE="$OUTPUT_DIR/evaluation_results.json"
    METRICS_FILE="$OUTPUT_DIR/metrics_summary.json"
    PLOTS_FILE="$OUTPUT_DIR/evaluation_plots.png"
    
    if [ -f "$METRICS_FILE" ]; then
        echo ""
        echo "üìã Results Summary:"
        echo "=================="
        
        # Extract key metrics using Python
        python -c "
import json
import sys
try:
    with open('$METRICS_FILE', 'r') as f:
        metrics = json.load(f)
    
    print(f'üéØ Mean Cosine Similarity: {metrics.get(\"cosine_similarity_mean\", 0):.4f}')
    print(f'üìä Median Cosine Similarity: {metrics.get(\"cosine_similarity_median\", 0):.4f}')
    print(f'üìä Std Deviation: {metrics.get(\"cosine_similarity_std\", 0):.4f}')
    print(f'üìä Min Similarity: {metrics.get(\"cosine_similarity_min\", 0):.4f}')
    print(f'üìä Max Similarity: {metrics.get(\"cosine_similarity_max\", 0):.4f}')
    print(f'')
    print(f'üèÜ Quality Distribution:')
    print(f'   High Quality (>0.7): {metrics.get(\"high_quality_percentage\", 0):.1f}%')
    print(f'   Very High Quality (>0.8): {metrics.get(\"very_high_quality_percentage\", 0):.1f}%')
    print(f'   Excellent Quality (>0.9): {metrics.get(\"excellent_quality_percentage\", 0):.1f}%')
    print(f'')
    print(f'üìä Additional Metrics:')
    print(f'   MSE Loss: {metrics.get(\"mse_loss\", 0):.6f}')
    print(f'   L1 Loss: {metrics.get(\"l1_loss\", 0):.6f}')
    print(f'   Token Similarity: {metrics.get(\"token_similarity_mean\", 0):.4f}')
    print(f'   Samples Evaluated: {metrics.get(\"num_samples\", 0):,}')
    print(f'   Evaluation Time: {metrics.get(\"evaluation_time_seconds\", 0):.1f} seconds')
    print(f'   Samples/Second: {metrics.get(\"samples_per_second\", 0):.1f}')
    
    # Performance assessment
    mean_sim = metrics.get('cosine_similarity_mean', 0)
    if mean_sim > 0.8:
        print(f'\\nüéâ ASSESSMENT: EXCELLENT performance!')
        print(f'   The model shows outstanding CLIP reproduction capability.')
    elif mean_sim > 0.6:
        print(f'\\n‚úÖ ASSESSMENT: VERY GOOD performance!')
        print(f'   The model demonstrates strong CLIP reproduction.')
    elif mean_sim > 0.4:
        print(f'\\nüëç ASSESSMENT: GOOD performance!')
        print(f'   The model shows solid CLIP reproduction capability.')
    elif mean_sim > 0.2:
        print(f'\\nüìà ASSESSMENT: FAIR performance!')
        print(f'   The model shows decent reproduction with room for improvement.')
    else:
        print(f'\\n‚ö†Ô∏è ASSESSMENT: Needs improvement.')
        print(f'   Consider training adjustments or architecture modifications.')
    
    # Denormalization status
    if metrics.get('denormalized', False):
        print(f'\\n‚úÖ CLIP denormalization was successfully applied.')
    else:
        print(f'\\n‚ö†Ô∏è CLIP denormalization was not applied - results may not reflect true performance.')
        
except Exception as e:
    print(f'Could not parse results: {e}')
    sys.exit(1)
"
        
        echo ""
        echo "üìÅ Results saved to: $OUTPUT_DIR"
        if [ -f "$RESULTS_FILE" ]; then
            echo "üìÑ Full results: $RESULTS_FILE"
        fi
        if [ -f "$PLOTS_FILE" ]; then
            echo "üìä Visualizations: $PLOTS_FILE"
        fi
    else
        echo "‚ö†Ô∏è No metrics file found"
        echo "Checking output directory contents:"
        ls -la "$OUTPUT_DIR" 2>/dev/null || echo "Output directory not found"
    fi
    
    echo ""
    echo "üéØ Interpretation Guide:"
    echo "======================"
    echo "‚Ä¢ Cosine Similarity Range: [-1, 1], higher is better"
    echo "‚Ä¢ >0.8: Excellent CLIP reproduction"
    echo "‚Ä¢ >0.6: Very good CLIP reproduction"
    echo "‚Ä¢ >0.4: Good CLIP reproduction"
    echo "‚Ä¢ >0.2: Fair CLIP reproduction"
    echo "‚Ä¢ <0.2: Needs improvement"
    echo ""
    echo "üìä Quality Thresholds:"
    echo "‚Ä¢ High Quality (>0.7): Images with strong semantic alignment"
    echo "‚Ä¢ Very High Quality (>0.8): Images with very strong alignment"
    echo "‚Ä¢ Excellent Quality (>0.9): Images with near-perfect alignment"
    
    echo ""
    echo "‚úÖ SUCCESS: MS-COCO evaluation completed!"
    
else
    echo "‚ùå FAILED: Evaluation exit code $EVAL_EXIT_CODE"
    echo ""
    echo "üí° Troubleshooting:"
    echo "==================="
    echo "‚Ä¢ Check log files in ./slurm_out/ for detailed error messages"
    echo "‚Ä¢ Verify model path and format:"
    echo "  - HuggingFace: config.json + model weights"
    echo "  - Checkpoint: checkpoint_step_*.pt files"
    echo "‚Ä¢ Ensure COCO dataset is properly downloaded and extracted"
    echo "‚Ä¢ Check GPU memory usage - try smaller batch size if OOM"
    echo "‚Ä¢ Verify all required Python packages are installed"
    echo ""
    echo "üîß Quick fixes:"
    echo "‚Ä¢ Reduce batch size: --batch_size 4"
    echo "‚Ä¢ Reduce samples: --max_samples 100"
    echo "‚Ä¢ Check model compatibility with current codebase"
fi

echo ""
echo "üèÅ Job completed at $(date)"
echo "========================================"

# Show final resource usage
echo ""
echo "üìä GPU Resource Usage Summary:"
nvidia-smi --query-gpu=name,memory.total,memory.used,utilization.gpu --format=csv,noheader,nounits | \
    awk 'BEGIN{print "GPU | Total Memory | Used Memory | Utilization"} {printf "%s | %s MB | %s MB | %s%%\n", $1, $2, $3, $4}'

echo ""
echo "üìö MS-COCO EVALUATION SUMMARY:"
echo "This job evaluates the trained BLIP3-o model on MS-COCO val2017 dataset."
echo "It measures how well the model can reproduce CLIP embeddings from EVA embeddings."
echo "The primary metric is cosine similarity between generated and ground truth CLIP embeddings."
echo "Results include quality distribution, statistical analysis, and visualizations."
echo "========================================"

exit $EVAL_EXIT_CODE