#!/bin/bash
#SBATCH --job-name=clip_repro_training
#SBATCH --partition=gpu_h100
#SBATCH --nodes=1
#SBATCH --gpus=1
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=8
#SBATCH --time=15:00:00
#SBATCH --mem=64G
#SBATCH --output=./slurm_out/clip_repro_%j.out
#SBATCH --error=./slurm_out/clip_repro_%j.err

# =============================================================================
# CLIP Reproduction Training with BLIP3-o DiT
# Task: Reproduce CLIP embeddings [B, N, 1024] from EVA embeddings [B, N, 4096]
# Key Feature: Minimal normalization approach (only for evaluation similarity)
# =============================================================================

echo "🚀 CLIP Reproduction Training with BLIP3-o DiT"
echo "==============================================="
echo "Job ID: ${SLURM_JOB_ID}"
echo "Node: $(hostname)"
echo "Time: $(date)"
echo "GPUs: $(nvidia-smi --query-gpu=name --format=csv,noheader,nounits | tr '\n' ', ')"
echo "==============================================="

cd $SLURM_SUBMIT_DIR

# Setup environment
module purge
module load 2024
module load Miniconda3/24.7.1-0
module load CUDA/12.6.0
source activate eva_clip_env

# Optional: WandB login (uncomment if using WandB)
# wandb login 0d9895af249ee18e4fa141e8a2350e0f4adb920f --relogin

# Configuration
EMBEDDINGS_DIR="/scratch-shared/azadaianchuk1/blip3o_workspace/embeddings/patch_only_256_tokens"
OUTPUT_DIR="./checkpoints/clip_repro_$(date +%Y%m%d_%H%M%S)"
TRAINING_MODE="patch_only"
MODEL_SIZE="base"

# Optimized hyperparameters for CLIP reproduction
NUM_EPOCHS=10                    # Conservative epochs for initial testing
BATCH_SIZE=8                   # Smaller batch size for stability
LEARNING_RATE=1e-4              # Conservative learning rate
WEIGHT_DECAY=0.01               # Standard regularization
WARMUP_STEPS=100                # Shorter warmup for testing
MAX_GRAD_NORM=1.0               # Gradient clipping for stability

# Evaluation parameters
EVAL_EVERY_N_STEPS=50
EVAL_NUM_SAMPLES=15
EVAL_INFERENCE_STEPS=20

# Testing configuration
OVERFIT_TEST_SIZE=20            # Enable overfitting test to verify architecture
DEBUG_MODE=""      # Enable debug logging
MAX_SHARDS=2                    # Start with fewer shards for testing

# Create output directory
mkdir -p "${OUTPUT_DIR}"
mkdir -p ./slurm_out

echo ""
echo "⚙️ CLIP Reproduction Configuration:"
echo "=================================="
echo "Task: Reproduce clean CLIP embeddings from EVA embeddings"
echo "Conditioning: EVA embeddings [B, N, 4096]"
echo "Target: CLIP embeddings [B, N, 1024]"
echo "Method: Rectified Flow Matching with BLIP3-o DiT"
echo "Normalization: MINIMAL (only for evaluation similarity)"
echo ""
echo "Embeddings: $EMBEDDINGS_DIR"
echo "Output: $OUTPUT_DIR"
echo "Training mode: $TRAINING_MODE"
echo "Model size: $MODEL_SIZE"
echo "Max shards: $MAX_SHARDS"
echo ""
echo "📊 Training Hyperparameters:"
echo "  Epochs: $NUM_EPOCHS"
echo "  Batch size: $BATCH_SIZE"
echo "  Learning rate: $LEARNING_RATE"
echo "  Weight decay: $WEIGHT_DECAY"
echo "  Warmup steps: $WARMUP_STEPS"
echo "  Max grad norm: $MAX_GRAD_NORM"
echo "  Overfitting test: $OVERFIT_TEST_SIZE samples"
echo ""
echo "🔍 Evaluation Configuration:"
echo "  Eval every: $EVAL_EVERY_N_STEPS steps"
echo "  Eval samples: $EVAL_NUM_SAMPLES"
echo "  Inference steps: $EVAL_INFERENCE_STEPS"
echo ""
echo "🚫 MINIMAL NORMALIZATION APPROACH:"
echo "  • No normalization during training"
echo "  • Raw embedding space learning"
echo "  • Normalization ONLY for cosine similarity in evaluation"
echo "  • Let model learn natural embedding distributions"
echo ""

# Verify embeddings exist
if [ ! -d "$EMBEDDINGS_DIR" ]; then
    echo "❌ Embeddings directory not found: $EMBEDDINGS_DIR"
    echo "Available embeddings:"
    ls -la "/scratch-shared/azadaianchuk1/blip3o_workspace/embeddings/" 2>/dev/null || echo "No embeddings found"
    exit 1
fi

echo "✅ Embeddings verified: $EMBEDDINGS_DIR"

# Check available shards
SHARD_COUNT=$(find "$EMBEDDINGS_DIR" -name "*.pkl" | wc -l)
echo "✅ Found $SHARD_COUNT embedding shards"

if [ $SHARD_COUNT -eq 0 ]; then
    echo "❌ No embedding shards found!"
    exit 1
fi

if [ $SHARD_COUNT -lt $MAX_SHARDS ]; then
    echo "⚠️ Only $SHARD_COUNT shards available (requested $MAX_SHARDS)"
    MAX_SHARDS=$SHARD_COUNT
    echo "   Adjusted to use $MAX_SHARDS shards"
fi



echo "✅ Training script found"

echo ""
echo "🚀 Starting CLIP Reproduction Training..."
echo "========================================"
echo "🧪 EXPECTED BEHAVIOR WITH MINIMAL NORMALIZATION:"
echo "  ✅ Non-zero gradients from first step"
echo "  ✅ Decreasing loss within first few epochs"
echo "  ✅ Velocity similarity increasing from ~0.01 to >0.1"
echo "  ✅ CLIP similarity during evaluation >0.1 (good), >0.4 (excellent)"
echo "  ✅ Overfitting test should achieve >0.8 similarity"
echo "  ✅ No NaN/Inf issues or tensor shape mismatches"
echo "  ✅ Raw embedding norms will vary naturally"
echo ""
echo "🏗️ ARCHITECTURE VALIDATION:"
echo "  • BLIP3-o DiT with 3D RoPE and Grouped-Query Attention"
echo "  • Sandwich Normalization (RMSNorm)"
echo "  • Rectified Flow Matching"
echo "  • Proper gradient flow and initialization"
echo "  • EVA [4096] → CLIP [1024] mapping"
echo ""

# Launch CLIP reproduction training
python train_dit.py \
    --chunked_embeddings_dir "$EMBEDDINGS_DIR" \
    --output_dir "$OUTPUT_DIR" \
    --model_size "$MODEL_SIZE" \
    --training_mode "$TRAINING_MODE" \
    --learning_rate $LEARNING_RATE \
    --batch_size $BATCH_SIZE \
    --num_epochs $NUM_EPOCHS \
    --warmup_steps $WARMUP_STEPS \
    --weight_decay $WEIGHT_DECAY \
    --max_grad_norm $MAX_GRAD_NORM \
    --eval_every_n_steps $EVAL_EVERY_N_STEPS \
    --eval_num_samples $EVAL_NUM_SAMPLES \
    --max_shards $MAX_SHARDS \
    --fp16 \
    $DEBUG_MODE

TRAINING_EXIT_CODE=$?

echo ""
echo "========================================"
echo "📊 CLIP Reproduction Results"
echo "========================================"

if [ $TRAINING_EXIT_CODE -eq 0 ]; then
    echo "✅ CLIP reproduction training completed successfully!"
    
    echo ""
    echo "📋 Training Summary:"
    echo "==================="
    
    # Check for training results
    SUMMARY_FILE="$OUTPUT_DIR/training_summary.json"
    CONFIG_FILE="$OUTPUT_DIR/experiment_config.json"
    
    if [ -f "$SUMMARY_FILE" ]; then
        echo ""
        echo "📊 Training Results:"
        echo "==================="
        
        # Extract key metrics using Python
        python -c "
import json
import sys
try:
    with open('$SUMMARY_FILE', 'r') as f:
        data = json.load(f)
    
    print(f'🎯 Best Loss: {data.get(\"best_loss\", float(\"inf\")):.6f}')
    print(f'🎯 Best CLIP Similarity: {data.get(\"best_eval_similarity\", 0):.4f}')
    print(f'📊 Total Steps: {data.get(\"total_steps\", 0):,}')
    print(f'⏱️ Training Time: {data.get(\"total_time_seconds\", 0):.1f} seconds')
    print(f'🚫 Minimal Normalization: {data.get(\"minimal_normalization\", False)}')
    
    # Overfitting test results
    if data.get('overfit_test', False):
        overfit_success = data.get('overfit_success', False)
        print(f'🧪 Overfitting Test: {\"✅ PASSED\" if overfit_success else \"❌ FAILED\"}')
        if overfit_success:
            print(f'   ✅ Architecture can learn - implementation is correct!')
        else:
            print(f'   ⚠️  Architecture struggles to overfit - check implementation')
    
    # Final evaluation
    final_eval = data.get('final_eval', {})
    if final_eval:
        print(f'')
        print(f'🔍 Final Evaluation:')
        clip_sim = final_eval.get('eval_clip_similarity', 0)
        high_qual = final_eval.get('eval_high_quality', 0) * 100
        very_high_qual = final_eval.get('eval_very_high_quality', 0) * 100
        
        print(f'   Overall CLIP Similarity: {clip_sim:.4f}')
        print(f'   High Quality (>0.7): {high_qual:.1f}%')
        print(f'   Very High Quality (>0.8): {very_high_qual:.1f}%')
        print(f'   Samples Evaluated: {final_eval.get(\"eval_samples\", 0):,}')
        
        # Assessment
        if clip_sim > 0.7:
            print(f'   🎉 EXCELLENT: Outstanding CLIP reproduction!')
        elif clip_sim > 0.4:
            print(f'   ✅ GOOD: Strong CLIP reproduction capability!')
        elif clip_sim > 0.1:
            print(f'   📈 FAIR: Decent reproduction, shows learning!')
        else:
            print(f'   ⚠️  NEEDS WORK: Low similarity, check hyperparameters')
    
except Exception as e:
    print(f'Could not parse training summary: {e}')
    # Try to show any available checkpoints
    import os
    checkpoints = [f for f in os.listdir('$OUTPUT_DIR') if f.endswith('.pt')]
    if checkpoints:
        print(f'Found {len(checkpoints)} checkpoint files')
        print(f'Latest: {max(checkpoints)}')
    sys.exit(1)
"
        
        echo ""
        echo "📁 Training artifacts saved to: $OUTPUT_DIR"
    else
        echo "⚠️ No training summary found, checking for any outputs..."
        echo "Directory contents:"
        ls -la "$OUTPUT_DIR" 2>/dev/null || echo "Output directory not found"
    fi
    
    echo ""
    echo "🎯 Next Steps:"
    echo "=============="
    echo "1. Review training logs above for success indicators"
    echo "2. If overfitting test passed, run full-scale training:"
    echo "   • Increase MAX_SHARDS to use more data" 
    echo "   • Increase NUM_EPOCHS for longer training"
    echo "   • Adjust BATCH_SIZE based on memory usage"
    echo "3. Compare with normalized approach if needed"
    echo "4. Run comprehensive evaluation on test set"
    echo ""
    echo "🔍 SUCCESS INDICATORS:"
    echo "  ✅ Non-zero gradients throughout training"
    echo "  ✅ Decreasing loss trend"
    echo "  ✅ Increasing similarity metrics"
    echo "  ✅ Overfitting test achieves >0.8 similarity"
    echo "  ✅ Final CLIP similarity >0.1 (good), >0.4 (excellent)"
    echo "  ✅ Natural embedding norm variation (not forced to 1.0)"
    
    echo ""
    echo "✅ SUCCESS: CLIP reproduction training completed!"
    
else
    echo "❌ FAILED: Training exit code $TRAINING_EXIT_CODE"
    echo ""
    echo "💡 Troubleshooting:"
    echo "  • Check log files in ./slurm_out/ for detailed error messages"
    echo "  • Verify all required Python files are present:"
    echo "    - train_clip_reproduction.py (main script)"
    echo "    - blip3o_clip_dit.py (BLIP3-o DiT model)"
    echo "    - blip3o_clip_loss.py (flow matching loss)"
    echo "    - blip3o_clip_dataset.py (data loading)"
    echo "    - blip3o_clip_trainer.py (training loop)"
    echo "  • Check embeddings directory structure and file formats"
    echo "  • Monitor GPU memory usage with nvidia-smi"
    echo "  • Try reducing batch_size if out-of-memory errors"
    echo ""
    echo "🔧 Quick Recovery Options:"
    echo "  • Start with overfitting test: --overfit_test_size 5 --batch_size 2"
    echo "  • Use smaller model: --model_size tiny"
    echo "  • Enable more debugging: --debug_mode --max_shards 1"
    echo "  • Check if embeddings are properly formatted"
fi

echo ""
echo "📊 GPU Resource Usage Summary:"
nvidia-smi --query-gpu=name,memory.total,memory.used,utilization.gpu --format=csv,noheader,nounits | \
    awk 'BEGIN{print "GPU | Total Memory | Used Memory | Utilization"} {printf "%s | %s MB | %s MB | %s%%\n", $1, $2, $3, $4}'

echo ""
echo "🏁 Job completed at $(date)"
echo "Total job time: $(echo "scale=2; ($(date +%s) - $SECONDS) / 3600" | bc -l) hours"
echo ""
echo "📚 CLIP REPRODUCTION SUMMARY:"
echo "This job tests BLIP3-o DiT architecture by reproducing CLIP embeddings from EVA embeddings."
echo "Success indicates the architecture can perform cross-modal embedding translation effectively."
echo "The minimal normalization approach tests if raw embedding spaces can be learned naturally."
echo "==============================================="

exit $TRAINING_EXIT_CODE