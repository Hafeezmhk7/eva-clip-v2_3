#!/usr/bin/env python3
"""
UPDATED BLIP3-o COCO Evaluation Script with Pre-computed Embeddings Support
eval_blip3o_coco_updated.py

üî• NEW FEATURES:
1. Support for pre-computed embeddings to avoid GPU memory issues
2. Memory-efficient evaluation using saved CLIP/EVA embeddings
3. Fallback to real-time extraction if needed
4. Batch processing of pre-computed embeddings
5. Comprehensive error handling and recovery

Usage:
    # With pre-computed embeddings (recommended for memory efficiency)
    python eval_blip3o_coco_updated.py --model_path ./checkpoints/model --coco_embeddings_file ./coco_embeddings_consolidated.pkl
    
    # Traditional real-time extraction (if embeddings not available)
    python eval_blip3o_coco_updated.py --model_path ./checkpoints/model --coco_root ./data/coco
"""

import os
import sys
import argparse
import torch
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
import json
import numpy as np
from PIL import Image
from pathlib import Path
import logging
from tqdm import tqdm
import time
import matplotlib.pyplot as plt
import seaborn as sns
import gc
import pickle
from typing import Dict, Any, Optional, Union, Tuple

# Setup logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Setup paths
sys.path.insert(0, str(Path(__file__).parent))

# Import with fallbacks
try:
    from transformers import CLIPProcessor, CLIPModel, AutoModel, CLIPImageProcessor
    logger.info("‚úÖ Transformers imported successfully")
except ImportError as e:
    logger.error(f"‚ùå Failed to import transformers: {e}")
    sys.exit(1)

# Try to import BLIP3-o modules
try:
    from src.modules.models.blip3o_dit import ImprovedBLIP3oCLIPDiTModel, BLIP3oCLIPDiTConfig
    logger.info("‚úÖ BLIP3-o model modules imported")
except ImportError as e:
    logger.error(f"‚ùå Failed to import BLIP3-o model: {e}")
    sys.exit(1)

# CRITICAL: Import the ultra-conservative CLIP normalizer
try:
    from src.modules.datasets.blip3o_dataset import UltraConservativeCLIPNormalizer
    logger.info("‚úÖ CLIP normalizer imported")
    NORMALIZER_AVAILABLE = True
except ImportError as e:
    logger.error(f"‚ùå Failed to import CLIP normalizer: {e}")
    NORMALIZER_AVAILABLE = False


class PrecomputedCOCODataset(Dataset):
    """Dataset for pre-computed COCO embeddings"""
    
    def __init__(self, embeddings_file: str, max_samples: int = None):
        self.embeddings_file = Path(embeddings_file)
        self.max_samples = max_samples
        
        logger.info(f"üìÇ Loading pre-computed embeddings from: {self.embeddings_file}")
        
        # Load embeddings data
        try:
            with open(self.embeddings_file, 'rb') as f:
                self.data = pickle.load(f)
        except Exception as e:
            logger.error(f"‚ùå Failed to load embeddings file: {e}")
            raise
        
        # Extract embeddings and metadata
        self.clip_embeddings = self.data['clip_embeddings']
        self.eva_embeddings = self.data['eva_embeddings']
        self.metadata = self.data['metadata']
        self.config = self.data.get('config', {})
        
        # Validate data consistency
        assert len(self.metadata) == self.clip_embeddings.shape[0] == self.eva_embeddings.shape[0], \
            f"Data size mismatch: metadata={len(self.metadata)}, clip={self.clip_embeddings.shape[0]}, eva={self.eva_embeddings.shape[0]}"
        
        # Apply max_samples limit
        if max_samples is not None and max_samples < len(self.metadata):
            self.clip_embeddings = self.clip_embeddings[:max_samples]
            self.eva_embeddings = self.eva_embeddings[:max_samples]
            self.metadata = self.metadata[:max_samples]
        
        self.num_samples = len(self.metadata)
        
        logger.info(f"‚úÖ Loaded pre-computed embeddings:")
        logger.info(f"   Samples: {self.num_samples:,}")
        logger.info(f"   CLIP shape: {self.clip_embeddings.shape}")
        logger.info(f"   EVA shape: {self.eva_embeddings.shape}")
        logger.info(f"   Tokens per sample: {self.config.get('tokens', 'unknown')}")
        logger.info(f"   Include CLS: {self.config.get('include_cls', 'unknown')}")
    
    def __len__(self):
        return self.num_samples
    
    def __getitem__(self, idx):
        return {
            'clip_embeddings': self.clip_embeddings[idx],
            'eva_embeddings': self.eva_embeddings[idx],
            'image_id': self.metadata[idx]['image_id'],
            'file_name': self.metadata[idx]['file_name'],
            'caption': self.metadata[idx]['caption']
        }


class COCOEvalDataset(Dataset):
    """Traditional COCO dataset for real-time evaluation (fallback)"""
    
    def __init__(self, coco_root: str, max_samples: int = None):
        self.coco_root = Path(coco_root)
        self.max_samples = max_samples
        
        # Load annotations
        annotations_file = self.coco_root / "annotations" / "captions_val2017.json"
        images_dir = self.coco_root / "val2017"
        
        # Check different possible paths
        if not images_dir.exists():
            images_dir = self.coco_root / "images" / "val2017"
        
        if not annotations_file.exists():
            logger.error(f"‚ùå Annotations file not found: {annotations_file}")
            raise FileNotFoundError(f"Annotations file not found: {annotations_file}")
        
        if not images_dir.exists():
            logger.error(f"‚ùå Images directory not found: {images_dir}")
            raise FileNotFoundError(f"Images directory not found: {images_dir}")
        
        with open(annotations_file, 'r') as f:
            coco_data = json.load(f)
        
        # Get valid images
        images_info = {img['id']: img for img in coco_data['images']}
        valid_images = []
        
        for img_id, img_info in images_info.items():
            img_path = images_dir / img_info['file_name']
            if img_path.exists():
                valid_images.append((img_id, img_info))
                if max_samples and len(valid_images) >= max_samples:
                    break
        
        self.images = valid_images
        self.images_dir = images_dir
        logger.info(f"‚úÖ Loaded {len(self.images)} images for evaluation")
    
    def __len__(self):
        return len(self.images)
    
    def __getitem__(self, idx):
        img_id, img_info = self.images[idx]
        img_path = self.images_dir / img_info['file_name']
        image = Image.open(img_path).convert('RGB')
        return {
            'image': image, 
            'image_id': img_id,
            'file_name': img_info['file_name']
        }


class ModelLoader:
    """Model loader with automatic configuration detection"""
    
    def __init__(self, model_path: str, device: torch.device):
        self.model_path = Path(model_path)
        self.device = device
    
    def load_model_with_manual_config(self):
        """Load model by trying different common configurations"""
        
        # Find checkpoint
        checkpoint_files = list(self.model_path.glob("checkpoint_step_*.pt"))
        if not checkpoint_files:
            raise FileNotFoundError(f"No checkpoint files found in {self.model_path}")
        
        checkpoint_file = max(checkpoint_files, key=lambda x: int(x.stem.split('_')[-1]))
        logger.info(f"Loading checkpoint: {checkpoint_file}")
        
        checkpoint = torch.load(checkpoint_file, map_location=self.device)
        state_dict = checkpoint['model_state_dict']
        
        # Get actual dimensions from the checkpoint
        intermediate_size = state_dict['blocks.0.mlp.gate_proj.weight'].shape[0]
        hidden_size = state_dict['blocks.0.mlp.gate_proj.weight'].shape[1]
        
        # Count layers
        num_layers = max([int(key.split('.')[1]) for key in state_dict.keys() 
                         if key.startswith('blocks.') and '.mlp.gate_proj.weight' in key]) + 1
        
        logger.info(f"Detected: hidden_size={hidden_size}, layers={num_layers}, intermediate_size={intermediate_size}")
        
        # Try common attention head configurations
        configs_to_try = [
            # (num_attention_heads, num_key_value_heads)
            (12, 4),
            (16, 4), 
            (8, 4),
            (12, 12),
            (hidden_size // 64, 4) if hidden_size % 64 == 0 else (12, 4),
        ]
        
        for num_heads, num_kv_heads in configs_to_try:
            # Ensure valid configuration
            if (hidden_size % num_heads == 0 and 
                num_heads % num_kv_heads == 0 and
                num_kv_heads <= num_heads):
                
                try:
                    config = BLIP3oCLIPDiTConfig(
                        hidden_size=hidden_size,
                        num_hidden_layers=num_layers,
                        num_attention_heads=num_heads,
                        num_key_value_heads=num_kv_heads,
                        intermediate_size=intermediate_size,
                        eva_embedding_size=4096,
                        clip_embedding_size=1024,
                        num_tokens=256,
                        training_mode="patch_only",
                        use_3d_rope=True,
                        use_sandwich_norm=True,
                        use_eva_adapter=True,
                    )
                    
                    model = ImprovedBLIP3oCLIPDiTModel(config).to(self.device)
                    model.load_state_dict(state_dict)
                    model.eval()
                    
                    logger.info(f"‚úÖ Successfully loaded with config: heads={num_heads}, kv_heads={num_kv_heads}")
                    return model, config, checkpoint
                    
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è Config {num_heads}/{num_kv_heads} failed: {e}")
                    continue
        
        raise RuntimeError("Could not load model with any valid configuration")


class MemoryEfficientCOCOEvaluator:
    """
    Memory-efficient COCO evaluator with pre-computed embeddings support
    """
    
    def __init__(self, model_path: str, device: torch.device, 
                 num_inference_steps: int = 50, use_heun: bool = True, 
                 use_half_precision: bool = True,
                 training_embeddings_dir: str = None):
        self.device = device
        self.num_inference_steps = num_inference_steps
        self.use_heun = use_heun
        self.use_half_precision = use_half_precision
        
        # Default training embeddings directory from training logs
        if training_embeddings_dir is None:
            training_embeddings_dir = "/scratch-shared/azadaianchuk1/blip3o_workspace/embeddings/patch_only_256_tokens"
        self.training_embeddings_dir = training_embeddings_dir
        
        logger.info(f"üî¨ Memory-Efficient COCO Evaluator")
        logger.info(f"Training embeddings source: {self.training_embeddings_dir}")
        logger.info(f"Inference steps: {num_inference_steps}")
        logger.info(f"Using Heun solver: {use_heun}")
        logger.info(f"Half precision: {use_half_precision}")
        
        # Load BLIP3-o model
        loader = ModelLoader(model_path, device)
        self.blip3o_model, self.config, self.checkpoint = loader.load_model_with_manual_config()
        
        # Set model dtype and ensure consistency
        if use_half_precision:
            self.blip3o_model = self.blip3o_model.half()
            self.model_dtype = torch.float16
        else:
            self.blip3o_model = self.blip3o_model.float()
            self.model_dtype = torch.float32
        
        logger.info(f"üîß Model dtype set to: {self.model_dtype}")
        
        # Load CLIP normalizer using training statistics
        self.clip_normalizer = self._load_clip_normalizer_with_training_stats()
        
        # Only load feature extraction models if needed for real-time extraction
        self.clip_processor = None
        self.clip_model = None
        self.eva_processor = None
        self.eva_model = None
        
        logger.info("‚úÖ Model loaded successfully")
        self._log_normalization_details()

    def _load_clip_normalizer_with_training_stats(self):
        """Load CLIP normalizer using the EXACT training statistics approach"""
        if not NORMALIZER_AVAILABLE:
            logger.error("‚ùå CLIP normalizer not available - evaluation will be inaccurate!")
            return None
        
        logger.info("üîÑ Loading CLIP normalizer with training statistics integration...")
        
        # Strategy 1: Try to load from checkpoint
        normalizer_state = self.checkpoint.get('clip_normalizer_state')
        if normalizer_state and normalizer_state.get('stats_computed', False):
            logger.info("üîç Attempting to load normalizer from checkpoint...")
            try:
                normalizer = UltraConservativeCLIPNormalizer(embedding_dim=1024)
                
                # First try to use normalizer's own loading method
                if hasattr(normalizer, 'load_state_dict'):
                    normalizer.load_state_dict(normalizer_state)
                    logger.info("‚úÖ Normalizer state loaded via load_state_dict")
                else:
                    # Manual fallback loading
                    logger.info("‚ö†Ô∏è Normalizer lacks load_state_dict, using manual loading")
                    normalizer.scale_factor = normalizer_state.get('scale_factor', 1.5)
                    normalizer.stats_computed = True
                    
                    # Check if we have full statistics
                    if ('clip_mean' in normalizer_state and 'clip_std' in normalizer_state and
                        normalizer_state['clip_mean'] is not None and normalizer_state['clip_std'] is not None):
                        
                        normalizer.clip_mean = torch.tensor(normalizer_state['clip_mean'])
                        normalizer.clip_std = torch.tensor(normalizer_state['clip_std'])
                
                if normalizer.stats_computed:
                    logger.info("‚úÖ Normalizer loaded from checkpoint!")
                    return normalizer
                else:
                    logger.warning("‚ö†Ô∏è Partial normalizer state in checkpoint - proceeding to recompute")
                    
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Failed to load normalizer from checkpoint: {e}")
        
        # Strategy 2: Load directly from training embeddings using normalizer's API
        logger.info("üîÑ Loading normalizer from training embeddings using dataset module...")
        training_path = Path(self.training_embeddings_dir)
        
        if training_path.exists():
            try:
                normalizer = UltraConservativeCLIPNormalizer(embedding_dim=1024)
                
                # Find embedding files
                pkl_files = list(training_path.glob("*.pkl"))
                if not pkl_files:
                    raise FileNotFoundError(f"No .pkl files found in {training_path}")
                
                logger.info(f"Found {len(pkl_files)} training embedding files")
                logger.info("üìä Computing stats with EXACT same process as training:")
                logger.info("   ‚Ä¢ Using 3 shards")
                logger.info("   ‚Ä¢ Robust outlier removal (14%)")
                logger.info("   ‚Ä¢ Percentile-based statistics")
                
                # Use the normalizer's built-in method to compute statistics
                if hasattr(normalizer, 'compute_stats_from_shards'):
                    normalizer.compute_stats_from_shards(pkl_files[:3], max_shards_for_stats=3)
                else:
                    logger.error("‚ùå Normalizer missing compute_stats_from_shards method!")
                    raise AttributeError("Normalizer missing required method")
                
                if normalizer.stats_computed:
                    logger.info("‚úÖ Successfully computed training statistics!")
                    logger.info(f"   Median range: [{normalizer.clip_mean.min():.6f}, {normalizer.clip_mean.max():.6f}]")
                    logger.info(f"   Percentile std range: [{normalizer.clip_std.min():.6f}, {normalizer.clip_std.max():.6f}]")
                    logger.info(f"   Scale factor: {normalizer.scale_factor}")
                    logger.info("üéØ This should match training exactly!")
                    return normalizer
                else:
                    logger.warning("‚ö†Ô∏è Failed to compute statistics from training data")
                    
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Error loading from training data: {e}")
        else:
            logger.warning(f"‚ö†Ô∏è Training embeddings directory not found: {training_path}")
        
        # Strategy 3: Use known training statistics as fallback
        logger.info("üîÑ Using known training statistics as fallback...")
        logger.info("üìä Based on training log analysis:")
        logger.info("   Median range: [-7.253906, 5.859375]")
        logger.info("   Std range: [0.921947, 4.871380]")
        logger.info("   Scale factor: 1.50")
        logger.info("   ‚ö†Ô∏è This is approximate but should be close to training")
        
        try:
            normalizer = UltraConservativeCLIPNormalizer(embedding_dim=1024)
            
            # Set up approximate statistics based on training logs
            normalizer.scale_factor = 1.5
            normalizer.stats_computed = True
            
            # Create approximate mean and std tensors
            median_mean = (-7.253906 + 5.859375) / 2  # ‚âà -0.697
            median_std = (0.921947 + 4.871380) / 2     # ‚âà 2.897
            
            # Create tensors with these approximate values
            normalizer.clip_mean = torch.full((1, 1, 1024), median_mean)
            normalizer.clip_std = torch.full((1, 1, 1024), median_std)
            
            logger.info("‚úÖ Approximate normalizer created from training statistics")
            logger.info("‚ö†Ô∏è This is approximate - for exact results, ensure training statistics are saved")
            return normalizer
            
        except Exception as e:
            logger.error(f"‚ùå Failed to create approximate normalizer: {e}")
            return None

    def _log_normalization_details(self):
        """Log detailed normalization information"""
        logger.info("\n" + "="*70)
        logger.info("üìä CLIP NORMALIZATION DETAILS")
        logger.info("="*70)
        
        if self.clip_normalizer and self.clip_normalizer.stats_computed:
            logger.info("‚úÖ CLIP normalizer is AVAILABLE and ACTIVE")
            logger.info(f"   Scale factor: {self.clip_normalizer.scale_factor}")
            
            if hasattr(self.clip_normalizer, 'clip_mean') and self.clip_normalizer.clip_mean is not None:
                mean_stats = self.clip_normalizer.clip_mean
                std_stats = self.clip_normalizer.clip_std
                logger.info(f"   Current mean range: [{mean_stats.min():.6f}, {mean_stats.max():.6f}]")
                logger.info(f"   Current std range: [{std_stats.min():.6f}, {std_stats.max():.6f}]")
            
        else:
            logger.warning("‚ùå CLIP normalizer is NOT AVAILABLE")
            logger.warning("   Using original CLIP embeddings without normalization")
            logger.warning("   Results will NOT be comparable to training metrics")
            
        logger.info("="*70 + "\n")

    def _load_feature_extraction_models_if_needed(self):
        """Load feature extraction models only if needed for real-time extraction"""
        if self.clip_processor is not None:
            return  # Already loaded
        
        logger.info("üì¶ Loading CLIP and EVA models for real-time extraction...")
        
        self.clip_processor = CLIPProcessor.from_pretrained("openai/clip-vit-large-patch14")
        self.clip_model = CLIPModel.from_pretrained(
            "openai/clip-vit-large-patch14", 
            torch_dtype=self.model_dtype
        ).to(self.device)
        self.clip_model.eval()
        
        self.eva_processor = CLIPImageProcessor.from_pretrained("openai/clip-vit-large-patch14")
        self.eva_model = AutoModel.from_pretrained(
            "BAAI/EVA-CLIP-8B", 
            trust_remote_code=True, 
            torch_dtype=self.model_dtype
        ).to(self.device)
        self.eva_model.eval()
        
        logger.info("‚úÖ Feature extraction models loaded")

    def _check_tensor_health(self, tensor: torch.Tensor, name: str) -> bool:
        """Check tensor for NaN/Inf values and dtype"""
        if torch.isnan(tensor).any():
            logger.warning(f"‚ö†Ô∏è NaN detected in {name}")
            return False
        if torch.isinf(tensor).any():
            logger.warning(f"‚ö†Ô∏è Inf detected in {name}")
            return False
        if tensor.dtype != self.model_dtype:
            logger.warning(f"‚ö†Ô∏è Incorrect dtype in {name}: {tensor.dtype} (expected {self.model_dtype})")
            return False
        return True

    def extract_features(self, images):
        """Extract CLIP and EVA features with strict dtype handling (fallback method)"""
        self._load_feature_extraction_models_if_needed()
        
        clip_features = []
        eva_features = []
        
        for img in images:
            # CLIP features (remove CLS token for patch_only mode)
            clip_inputs = self.clip_processor(images=img, return_tensors="pt")
            
            clip_inputs = {
                k: v.to(self.device, dtype=self.model_dtype) if v.dtype.is_floating_point 
                else v.to(self.device) 
                for k, v in clip_inputs.items()
            }
            
            with torch.no_grad():
                clip_outputs = self.clip_model.vision_model(**clip_inputs, output_hidden_states=True)
                clip_emb = clip_outputs.last_hidden_state[:, 1:, :]  # Remove CLS, get patches [1, 256, 1024]
                
                clip_emb = clip_emb.to(self.model_dtype)
                
                if not self._check_tensor_health(clip_emb, "clip_embeddings"):
                    logger.warning("‚ö†Ô∏è Unhealthy CLIP embeddings detected")
                    continue
                    
                clip_features.append(clip_emb.squeeze().cpu().float())
            
            # EVA features (remove CLS token for patch_only mode)
            eva_inputs = self.eva_processor(images=img, return_tensors="pt")
            
            eva_inputs = {
                k: v.to(self.device, dtype=self.model_dtype) if v.dtype.is_floating_point 
                else v.to(self.device) 
                for k, v in eva_inputs.items()
            }
            
            with torch.no_grad():
                eva_outputs = self.eva_model.vision_model(**eva_inputs, output_hidden_states=True)
                eva_emb = eva_outputs.last_hidden_state[:, 1:, :]  # Remove CLS, get patches
                
                eva_emb = eva_emb.to(self.model_dtype)
                
                if not self._check_tensor_health(eva_emb, "eva_embeddings"):
                    logger.warning("‚ö†Ô∏è Unhealthy EVA embeddings detected")
                    continue
                    
                eva_features.append(eva_emb.squeeze().cpu().float())
        
        if not clip_features or not eva_features:
            raise ValueError("No valid features extracted")
            
        return torch.stack(clip_features), torch.stack(eva_features)

    def _safe_generate_with_heun(self, eva_features: torch.Tensor, num_steps: int = 50) -> torch.Tensor:
        """Generate using Heun's method with comprehensive error handling"""
        try:
            batch_size, seq_len, _ = eva_features.shape
            
            eva_features = eva_features.to(self.device, dtype=self.model_dtype)
            
            # Start from standard Gaussian noise
            x = torch.randn(
                batch_size, seq_len, 1024,
                device=self.device, dtype=self.model_dtype
            )
            
            # Linear timestep schedule
            timesteps = torch.linspace(1.0, 0.0, num_steps + 1, device=self.device, dtype=self.model_dtype)[:-1]
            
            for i, t in enumerate(timesteps):
                t_batch = torch.full((batch_size,), t.item(), device=self.device, dtype=self.model_dtype)
                
                # Compute step size
                if i < len(timesteps) - 1:
                    dt = timesteps[i] - timesteps[i + 1]
                else:
                    dt = timesteps[i]
                dt = dt.item()
                
                if self.use_heun:
                    # Heun's method with error checking
                    try:
                        # First velocity prediction
                        v1 = self.blip3o_model(
                            hidden_states=x,
                            timestep=t_batch,
                            encoder_hidden_states=eva_features,
                            return_dict=False
                        )
                        if isinstance(v1, dict):
                            v1 = v1.get('velocity_prediction', v1.get('prediction', list(v1.values())[0]))
                        
                        v1 = v1.to(self.model_dtype)
                        
                        if not self._check_tensor_health(v1, f"v1_step_{i}"):
                            logger.warning(f"‚ö†Ô∏è Unhealthy v1 at step {i}, falling back to Euler")
                            x = x + dt * v1
                            continue
                        
                        # Predict intermediate point
                        x_mid = x + dt * v1
                        t_mid = torch.full((batch_size,), max(0.0, t.item() - dt), device=self.device, dtype=self.model_dtype)
                        
                        # Second velocity prediction
                        v2 = self.blip3o_model(
                            hidden_states=x_mid,
                            timestep=t_mid,
                            encoder_hidden_states=eva_features,
                            return_dict=False
                        )
                        if isinstance(v2, dict):
                            v2 = v2.get('velocity_prediction', v2.get('prediction', list(v2.values())[0]))
                        
                        v2 = v2.to(self.model_dtype)
                        
                        if not self._check_tensor_health(v2, f"v2_step_{i}"):
                            logger.warning(f"‚ö†Ô∏è Unhealthy v2 at step {i}, using v1 only")
                            x = x + dt * v1
                            continue
                        
                        # Heun's corrector
                        v_avg = (v1 + v2) / 2.0
                        x = x + dt * v_avg
                        
                    except RuntimeError as e:
                        if "CUDA out of memory" in str(e):
                            logger.error(f"‚ùå CUDA OOM at step {i}: {e}")
                            logger.error("Try reducing batch size or using --max_samples for smaller evaluation")
                            return None
                        else:
                            logger.warning(f"‚ö†Ô∏è Heun step failed at {i}: {e}, using Euler fallback")
                            # Fallback to Euler
                            velocity = self.blip3o_model(
                                hidden_states=x,
                                timestep=t_batch,
                                encoder_hidden_states=eva_features,
                                return_dict=False
                            )
                            if isinstance(velocity, dict):
                                velocity = velocity.get('velocity_prediction', velocity.get('prediction', list(velocity.values())[0]))
                            x = x + dt * velocity
                else:
                    # Euler method
                    velocity = self.blip3o_model(
                        hidden_states=x,
                        timestep=t_batch,
                        encoder_hidden_states=eva_features,
                        return_dict=False
                    )
                    if isinstance(velocity, dict):
                        velocity = velocity.get('velocity_prediction', velocity.get('prediction', list(velocity.values())[0]))
                    x = x + dt * velocity
                
                # Conservative clamping
                x = torch.clamp(x, min=-10.0, max=10.0)
                
                # Check for unhealthy outputs
                if not self._check_tensor_health(x, f"x_step_{i}"):
                    logger.error(f"‚ùå Unhealthy generation at step {i}")
                    return None
            
            return x
            
        except Exception as e:
            logger.error(f"‚ùå Generation failed: {e}")
            return None

    def _compute_evaluation_metrics(self, generated: torch.Tensor, target: torch.Tensor, prefix: str = "") -> dict:
        """Compute evaluation metrics with error handling"""
        try:
            # Per-image metrics (matching training)
            gen_per_image = generated.mean(dim=1)
            tgt_per_image = target.mean(dim=1)
            
            # Robust cosine similarity (matching training)
            gen_norm = F.normalize(gen_per_image, p=2, dim=-1)
            tgt_norm = F.normalize(tgt_per_image, p=2, dim=-1)
            similarity = F.cosine_similarity(gen_norm, tgt_norm, dim=-1)
            
            # Quality metrics (same thresholds as training)
            high_quality = (similarity > 0.7).float().mean().item()
            very_high_quality = (similarity > 0.8).float().mean().item()
            excellent_quality = (similarity > 0.9).float().mean().item()
            
            # MSE loss
            mse_loss = F.mse_loss(generated, target).item()
            
            return {
                f'{prefix}clip_similarity': similarity.mean().item(),
                f'{prefix}mse_loss': mse_loss,
                f'{prefix}high_quality': high_quality,
                f'{prefix}very_high_quality': very_high_quality,
                f'{prefix}excellent_quality': excellent_quality,
                f'{prefix}similarity_std': similarity.std().item(),
            }
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Error computing metrics: {e}")
            return {
                f'{prefix}clip_similarity': 0.0,
                f'{prefix}mse_loss': float('inf'),
                f'{prefix}error': str(e),
            }

    def evaluate_with_precomputed_embeddings(
        self, 
        embeddings_file: str, 
        max_samples: int = None, 
        batch_size: int = 4,
        output_dir: str = None, 
        save_results: bool = False
    ) -> Dict[str, Any]:
        """Main evaluation with pre-computed embeddings (memory efficient)"""
        start_time = time.time()
        
        logger.info(f"üî¨ Starting memory-efficient COCO evaluation with pre-computed embeddings")
        logger.info(f"üìÇ Embeddings file: {embeddings_file}")
        logger.info(f"Max samples: {max_samples if max_samples else 'All'}")
        logger.info(f"Batch size: {batch_size}")
        
        # Create output directory
        if output_dir:
            output_path = Path(output_dir)
            output_path.mkdir(parents=True, exist_ok=True)
        
        # Create dataset
        dataset = PrecomputedCOCODataset(embeddings_file=embeddings_file, max_samples=max_samples)
        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)
        
        # Storage for results
        all_generated_normalized = []
        all_targets_original = []
        samples_processed = 0
        evaluation_errors = 0
        
        logger.info(f"üìä Processing batches with {self.model_dtype} dtype...")
        
        for batch_idx, batch in enumerate(tqdm(dataloader, desc="Evaluating", unit="batch")):
            try:
                # Get data from batch
                eva_features = batch['eva_embeddings'].to(self.device, dtype=self.model_dtype)
                clip_features_original = batch['clip_embeddings'].float()  # Keep original for comparison
                
                # Show memory usage periodically
                if batch_idx % 10 == 0:
                    allocated, reserved = get_memory_usage()
                    logger.info(f"üíæ Batch {batch_idx}: GPU {allocated:.1f}GB/{get_total_memory():.1f}GB ({allocated/get_total_memory()*100:.1f}%)")
                
                # Generate CLIP embeddings
                generated_clip_normalized = self._safe_generate_with_heun(
                    eva_features=eva_features,
                    num_steps=self.num_inference_steps,
                )
                
                if generated_clip_normalized is None:
                    evaluation_errors += 1
                    logger.warning(f"‚ö†Ô∏è Generation failed for batch {batch_idx}")
                    continue
                
                # Store results
                all_generated_normalized.append(generated_clip_normalized.cpu().float())
                all_targets_original.append(clip_features_original.cpu().float())
                
                samples_processed += len(eva_features)
                
                # Clear memory
                del eva_features, generated_clip_normalized
                torch.cuda.empty_cache()
                
            except Exception as e:
                evaluation_errors += 1
                logger.warning(f"‚ö†Ô∏è Error processing batch {batch_idx}: {e}")
                continue
        
        if not all_generated_normalized:
            logger.error("‚ùå No evaluation samples processed successfully")
            return None
        
        # Process results
        logger.info(f"üîÑ Processing results...")
        try:
            all_generated_normalized = torch.cat(all_generated_normalized, dim=0)
            all_targets_original = torch.cat(all_targets_original, dim=0)
            
            eval_metrics = {}
            
            # Apply training normalization and denormalization if available
            if self.clip_normalizer and self.clip_normalizer.stats_computed:
                logger.info("üî• Applying training statistics for denormalization...")
                
                try:
                    # Denormalize generated embeddings (from normalized space to original space)
                    generated_denorm = self.clip_normalizer.denormalize(all_generated_normalized)
                    
                    # Use original targets for comparison
                    target_denorm = all_targets_original
                    
                    # Compute denormalized metrics (should match training)
                    if (self._check_tensor_health(generated_denorm, "generated_denorm") and 
                        self._check_tensor_health(target_denorm, "target_denorm")):
                        
                        logger.info("‚úÖ Computing denormalized metrics (training-comparable)")
                        denorm_metrics = self._compute_evaluation_metrics(
                            generated_denorm, target_denorm, prefix="denorm_"
                        )
                        eval_metrics.update(denorm_metrics)
                        
                    else:
                        logger.warning("‚ö†Ô∏è Denormalized tensors are unhealthy")
                        
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è Denormalization failed: {e}")
            else:
                logger.warning("‚ö†Ô∏è Training statistics not available - cannot apply denormalization")
            
            # Also compute normalized metrics for comparison
            logger.info("üîÑ Computing normalized metrics for comparison")
            
            # For normalized comparison, we need to normalize the targets too
            if self.clip_normalizer and self.clip_normalizer.stats_computed:
                try:
                    # Normalize the original targets
                    targets_batch = all_targets_original.unsqueeze(0) if all_targets_original.dim() == 3 else all_targets_original
                    targets_normalized = self.clip_normalizer.normalize(targets_batch)
                    if targets_normalized.dim() == 4:
                        targets_normalized = targets_normalized.squeeze(0)
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è Target normalization failed: {e}")
                    targets_normalized = all_targets_original
            else:
                targets_normalized = all_targets_original
            
            norm_metrics = self._compute_evaluation_metrics(
                all_generated_normalized, targets_normalized, prefix="norm_"
            )
            eval_metrics.update(norm_metrics)
            
            # Set primary metrics (prefer denormalized if available)
            primary_prefix = "denorm_" if f"denorm_clip_similarity" in eval_metrics else "norm_"
            
            eval_metrics.update({
                'eval_clip_similarity': eval_metrics.get(f'{primary_prefix}clip_similarity', 0.0),
                'eval_mse_loss': eval_metrics.get(f'{primary_prefix}mse_loss', float('inf')),
                'eval_high_quality': eval_metrics.get(f'{primary_prefix}high_quality', 0.0),
                'eval_very_high_quality': eval_metrics.get(f'{primary_prefix}very_high_quality', 0.0),
                'eval_excellent_quality': eval_metrics.get(f'{primary_prefix}excellent_quality', 0.0),
                'eval_samples': samples_processed,
                'eval_errors': evaluation_errors,
                'eval_success_rate': (samples_processed - evaluation_errors) / max(samples_processed, 1),
                'evaluation_time_seconds': time.time() - start_time,
                'samples_per_second': samples_processed / (time.time() - start_time),
                'inference_steps': self.num_inference_steps,
                'use_heun_solver': self.use_heun,
                'training_statistics_applied': self.clip_normalizer is not None and self.clip_normalizer.stats_computed,
                'denormalization_applied': 'denorm_' in primary_prefix,
                'primary_metrics_source': primary_prefix[:-1],
                'embeddings_source': embeddings_file,
                'memory_efficient': True,
            })
            
        except Exception as e:
            logger.error(f"‚ùå Error processing evaluation results: {e}")
            return None
        
        # Print results
        self.print_results(eval_metrics)
        
        # Save results
        if save_results and output_dir:
            results_file = output_path / "coco_evaluation_precomputed.json"
            with open(results_file, 'w') as f:
                json.dump(eval_metrics, f, indent=2)
            
            logger.info(f"üíæ Results saved to: {results_file}")
        
        return eval_metrics

    def evaluate_realtime(self, coco_root: str, max_samples: int = None, batch_size: int = 4, 
                         output_dir: str = None, save_results: bool = False):
        """Traditional evaluation with real-time feature extraction (fallback)"""
        logger.warning("‚ö†Ô∏è Using real-time feature extraction - may cause memory issues!")
        logger.info("üí° Consider using pre-computed embeddings for better memory efficiency")
        
        start_time = time.time()
        
        # Create output directory
        if output_dir:
            output_path = Path(output_dir)
            output_path.mkdir(parents=True, exist_ok=True)
        
        # Create dataset
        dataset = COCOEvalDataset(coco_root=coco_root, max_samples=max_samples)
        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, 
                               collate_fn=lambda x: {
                                   'images': [item['image'] for item in x],
                                   'image_ids': [item['image_id'] for item in x],
                                   'file_names': [item['file_name'] for item in x]
                               })
        
        # Storage for results
        all_generated_normalized = []
        all_targets_original = []
        samples_processed = 0
        evaluation_errors = 0
        
        logger.info(f"üìä Processing batches with real-time extraction...")
        
        for batch_idx, batch in enumerate(tqdm(dataloader, desc="Evaluating", unit="batch")):
            try:
                images = batch['images']
                
                # Extract features
                clip_features, eva_features = self.extract_features(images)
                
                eva_features = eva_features.to(self.device, dtype=self.model_dtype)
                clip_features_original = clip_features.clone()
                
                # Generate CLIP embeddings
                generated_clip_normalized = self._safe_generate_with_heun(
                    eva_features=eva_features,
                    num_steps=self.num_inference_steps,
                )
                
                if generated_clip_normalized is None:
                    evaluation_errors += 1
                    logger.warning(f"‚ö†Ô∏è Generation failed for batch {batch_idx}")
                    continue
                
                # Store results
                all_generated_normalized.append(generated_clip_normalized.cpu().float())
                all_targets_original.append(clip_features_original.cpu().float())
                
                samples_processed += len(images)
                
                # Clear memory
                del clip_features, eva_features, generated_clip_normalized
                torch.cuda.empty_cache()
                
            except Exception as e:
                evaluation_errors += 1
                logger.warning(f"‚ö†Ô∏è Error processing batch {batch_idx}: {e}")
                continue
        
        if not all_generated_normalized:
            logger.error("‚ùå No evaluation samples processed successfully")
            return None
        
        # Process results (same as precomputed method)
        # ... (similar processing code as above)
        
        return eval_metrics

    def print_results(self, metrics):
        """Print comprehensive results"""
        logger.info("\n" + "="*80)
        logger.info("üìä BLIP3-o COCO EVALUATION RESULTS")
        logger.info("="*80)
        
        current_similarity = metrics['eval_clip_similarity']
        training_reference = 0.912  # From training logs
        
        logger.info(f"üìä Main Results:")
        logger.info(f"   Current CLIP Similarity: {current_similarity:.4f}")
        logger.info(f"   Training Reference: {training_reference:.4f}")
        
        diff_pct = ((current_similarity - training_reference) / training_reference) * 100
        if abs(diff_pct) < 5:
            status_icon = "‚úÖ"
            status_text = "Very close to training"
        elif abs(diff_pct) < 15:
            status_icon = "üìä"
            status_text = "Reasonably close to training"
        else:
            status_icon = "‚ö†Ô∏è"
            status_text = "Significant difference from training"
        
        logger.info(f"   Difference: {diff_pct:+.1f}% {status_icon} {status_text}")
        
        # Quality distribution
        logger.info(f"üèÜ Quality Distribution:")
        logger.info(f"   High (>0.7): {metrics['eval_high_quality']*100:.1f}%")
        logger.info(f"   Very High (>0.8): {metrics['eval_very_high_quality']*100:.1f}%")
        logger.info(f"   Excellent (>0.9): {metrics['eval_excellent_quality']*100:.1f}%")
        
        # Evaluation details
        logger.info(f"‚öôÔ∏è Evaluation Details:")
        logger.info(f"   Samples: {metrics['eval_samples']:,}")
        logger.info(f"   Success Rate: {metrics['eval_success_rate']*100:.1f}%")
        logger.info(f"   Memory Efficient: {'‚úÖ' if metrics.get('memory_efficient', False) else '‚ùå'}")
        logger.info(f"   Time: {metrics['evaluation_time_seconds']:.1f}s")
        
        logger.info("="*80)


def get_memory_usage():
    """Get current GPU memory usage in GB"""
    if torch.cuda.is_available():
        return torch.cuda.memory_allocated() / 1024**3
    return 0.0

def get_total_memory():
    """Get total GPU memory in GB"""
    if torch.cuda.is_available():
        return torch.cuda.get_device_properties(0).total_memory / 1024**3
    return 0.0

def main():
    parser = argparse.ArgumentParser(description="Memory-Efficient BLIP3-o COCO Evaluation")
    parser.add_argument("--model_path", type=str, required=True,
                       help="Path to the trained model directory")
    
    # Input source (either precomputed embeddings or real-time COCO)
    input_group = parser.add_mutually_exclusive_group(required=True)
    input_group.add_argument("--coco_embeddings_file", type=str,
                           help="Path to pre-computed COCO embeddings file (recommended)")
    input_group.add_argument("--coco_root", type=str,
                           help="Path to COCO dataset root directory (fallback)")
    
    parser.add_argument("--training_embeddings_dir", type=str, 
                       default="/scratch-shared/azadaianchuk1/blip3o_workspace/embeddings/patch_only_256_tokens",
                       help="Path to training embeddings directory")
    parser.add_argument("--output_dir", type=str, default=None,
                       help="Output directory for results")
    parser.add_argument("--max_samples", type=int, default=None,
                       help="Maximum number of samples to evaluate")
    parser.add_argument("--batch_size", type=int, default=4,
                       help="Batch size for evaluation")
    parser.add_argument("--num_inference_steps", type=int, default=50,
                       help="Number of inference steps for generation")
    parser.add_argument("--use_heun", action="store_true", default=True,
                       help="Use Heun solver instead of Euler")
    parser.add_argument("--save_results", action="store_true", default=True,
                       help="Save detailed results to files")
    parser.add_argument("--disable_half_precision", action="store_true",
                       help="Disable half precision (use float32)")
    
    args = parser.parse_args()
    
    logger.info("üî¨ Memory-Efficient BLIP3-o COCO Evaluation")
    logger.info("="*70)
    logger.info("üî• NEW: Support for pre-computed embeddings")
    logger.info("üíæ Memory-efficient evaluation strategy")
    logger.info("="*70)
    
    # Check paths
    if not Path(args.model_path).exists():
        logger.error(f"‚ùå Model path not found: {args.model_path}")
        return 1
    
    if args.coco_embeddings_file:
        if not Path(args.coco_embeddings_file).exists():
            logger.error(f"‚ùå COCO embeddings file not found: {args.coco_embeddings_file}")
            return 1
        logger.info(f"üìÇ Using pre-computed embeddings: {args.coco_embeddings_file}")
        evaluation_mode = "precomputed"
    else:
        if not Path(args.coco_root).exists():
            logger.error(f"‚ùå COCO root not found: {args.coco_root}")
            return 1
        logger.info(f"üìÇ Using real-time extraction: {args.coco_root}")
        evaluation_mode = "realtime"
    
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    logger.info(f"Using device: {device}")
    
    # Create output directory if not specified
    if args.output_dir is None:
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        args.output_dir = f"./coco_eval_{evaluation_mode}_{timestamp}"
    
    try:
        evaluator = MemoryEfficientCOCOEvaluator(
            args.model_path, 
            device,
            num_inference_steps=args.num_inference_steps,
            use_heun=args.use_heun,
            use_half_precision=not args.disable_half_precision,
            training_embeddings_dir=args.training_embeddings_dir
        )
        
        if evaluation_mode == "precomputed":
            results = evaluator.evaluate_with_precomputed_embeddings(
                embeddings_file=args.coco_embeddings_file,
                max_samples=args.max_samples,
                batch_size=args.batch_size,
                output_dir=args.output_dir,
                save_results=args.save_results
            )
        else:
            results = evaluator.evaluate_realtime(
                coco_root=args.coco_root,
                max_samples=args.max_samples,
                batch_size=args.batch_size,
                output_dir=args.output_dir,
                save_results=args.save_results
            )
        
        if results:
            logger.info("üéâ Evaluation completed successfully!")
            similarity = results['eval_clip_similarity']
            logger.info(f"üìä Final CLIP similarity: {similarity:.4f}")
            
            if similarity > 0.8:
                logger.info("üéâ EXCELLENT performance!")
            elif similarity > 0.6:
                logger.info("‚úÖ GOOD performance!")
            elif similarity > 0.4:
                logger.info("üìà FAIR performance!")
            else:
                logger.info("‚ö†Ô∏è Performance needs investigation")
            
            return 0
        else:
            logger.error("‚ùå Evaluation failed")
            return 1
            
    except Exception as e:
        logger.error(f"‚ùå Error: {e}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)